{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from ldm.util import instantiate_from_config\n",
    "import torch\n",
    "import json\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../../models/sd/sd-v1-4.ckpt\n",
      "Global Step: 470000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "make_beta_schedule:linear\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.position_ids', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "inference_file = \"./ProSpect_Classifier/data/CUB'/novel.json\"       ### what inference file we are using \n",
    "\n",
    "### Loading the model to the config \n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config=\"../../configs/stable-diffusion/v1-inference.yaml\"\n",
    "ckpt=\"../../models/sd/sd-v1-4.ckpt\"\n",
    "config = OmegaConf.load(f\"{config}\")\n",
    "model = load_model_from_config(config, f\"{ckpt}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = torch.load(\"../../ProSpect_Classifier/logs/class_embeddings_l2_21.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_list_0 = []\n",
    "for i in condition:\n",
    "    condition_list_0.append(condition[i])\n",
    "    \n",
    "condition_list=torch.stack(condition_list_0,dim=0)\n",
    "condition_list=condition_list.transpose(0,1)\n",
    "condition_list.shape\n",
    "\n",
    "\n",
    "\n",
    "temp=condition_list.unsqueeze(2).repeat(1,1,7,1,1)\n",
    "x= F.cosine_similarity(temp,temp.transpose(1,2),dim=-1)\n",
    "x=x.mean(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.2169, 0.2944, 0.1946, 0.2718, 0.2641, 0.2399],\n",
       "         [0.2169, 1.0000, 0.2441, 0.3328, 0.4282, 0.2920, 0.2742],\n",
       "         [0.2944, 0.2441, 1.0000, 0.3478, 0.3342, 0.2168, 0.1938],\n",
       "         [0.1946, 0.3328, 0.3478, 1.0000, 0.3746, 0.3116, 0.2132],\n",
       "         [0.2718, 0.4282, 0.3342, 0.3746, 1.0000, 0.2903, 0.1650],\n",
       "         [0.2641, 0.2920, 0.2168, 0.3116, 0.2903, 1.0000, 0.3133],\n",
       "         [0.2399, 0.2742, 0.1938, 0.2132, 0.1650, 0.3133, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.2295, 0.3684, 0.4005, 0.3388, 0.2780, 0.2730],\n",
       "         [0.2295, 1.0000, 0.2911, 0.2231, 0.2595, 0.3335, 0.3165],\n",
       "         [0.3684, 0.2911, 1.0000, 0.4309, 0.3502, 0.3969, 0.3884],\n",
       "         [0.4005, 0.2231, 0.4309, 1.0000, 0.3631, 0.3123, 0.3795],\n",
       "         [0.3388, 0.2595, 0.3502, 0.3631, 1.0000, 0.2900, 0.2220],\n",
       "         [0.2780, 0.3335, 0.3969, 0.3123, 0.2900, 1.0000, 0.4899],\n",
       "         [0.2730, 0.3165, 0.3884, 0.3795, 0.2220, 0.4899, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.3815, 0.2447, 0.2722, 0.3850, 0.1746, 0.2348],\n",
       "         [0.3815, 1.0000, 0.3503, 0.3912, 0.4276, 0.2474, 0.3095],\n",
       "         [0.2447, 0.3503, 1.0000, 0.6180, 0.3506, 0.5339, 0.5888],\n",
       "         [0.2722, 0.3912, 0.6180, 1.0000, 0.3909, 0.5163, 0.6194],\n",
       "         [0.3850, 0.4276, 0.3506, 0.3909, 1.0000, 0.3591, 0.2950],\n",
       "         [0.1746, 0.2474, 0.5339, 0.5163, 0.3591, 1.0000, 0.5680],\n",
       "         [0.2348, 0.3095, 0.5888, 0.6194, 0.2950, 0.5680, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.4376, 0.4906, 0.4427, 0.4665, 0.4294, 0.4659],\n",
       "         [0.4376, 1.0000, 0.6009, 0.4613, 0.4558, 0.5075, 0.4652],\n",
       "         [0.4906, 0.6009, 1.0000, 0.4296, 0.4224, 0.6459, 0.5267],\n",
       "         [0.4427, 0.4613, 0.4296, 1.0000, 0.4218, 0.3594, 0.4008],\n",
       "         [0.4665, 0.4558, 0.4224, 0.4218, 1.0000, 0.3399, 0.3849],\n",
       "         [0.4294, 0.5075, 0.6459, 0.3594, 0.3399, 1.0000, 0.4918],\n",
       "         [0.4659, 0.4652, 0.5267, 0.4008, 0.3849, 0.4918, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.2519, 0.3901, 0.3167, 0.2980, 0.2472, 0.2766],\n",
       "         [0.2519, 1.0000, 0.5235, 0.4487, 0.4330, 0.5543, 0.4632],\n",
       "         [0.3901, 0.5235, 1.0000, 0.5365, 0.5083, 0.5424, 0.4139],\n",
       "         [0.3167, 0.4487, 0.5365, 1.0000, 0.4685, 0.3926, 0.4084],\n",
       "         [0.2980, 0.4330, 0.5083, 0.4685, 1.0000, 0.3563, 0.3808],\n",
       "         [0.2472, 0.5543, 0.5424, 0.3926, 0.3563, 1.0000, 0.4616],\n",
       "         [0.2766, 0.4632, 0.4139, 0.4084, 0.3808, 0.4616, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.3653, 0.3839, 0.4306, 0.3318, 0.2961, 0.4416],\n",
       "         [0.3653, 1.0000, 0.2940, 0.4000, 0.3479, 0.3258, 0.3892],\n",
       "         [0.3839, 0.2940, 1.0000, 0.4357, 0.4692, 0.3183, 0.3878],\n",
       "         [0.4306, 0.4000, 0.4357, 1.0000, 0.3349, 0.2350, 0.3827],\n",
       "         [0.3318, 0.3479, 0.4692, 0.3349, 1.0000, 0.2653, 0.3661],\n",
       "         [0.2961, 0.3258, 0.3183, 0.2350, 0.2653, 1.0000, 0.3023],\n",
       "         [0.4416, 0.3892, 0.3878, 0.3827, 0.3661, 0.3023, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.4288, 0.4809, 0.4603, 0.3444, 0.1951, 0.2873],\n",
       "         [0.4288, 1.0000, 0.5642, 0.5542, 0.4294, 0.2715, 0.2996],\n",
       "         [0.4809, 0.5642, 1.0000, 0.6316, 0.5043, 0.3153, 0.3473],\n",
       "         [0.4603, 0.5542, 0.6316, 1.0000, 0.4306, 0.2526, 0.2972],\n",
       "         [0.3444, 0.4294, 0.5043, 0.4306, 1.0000, 0.2713, 0.2282],\n",
       "         [0.1951, 0.2715, 0.3153, 0.2526, 0.2713, 1.0000, 0.2846],\n",
       "         [0.2873, 0.2996, 0.3473, 0.2972, 0.2282, 0.2846, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.3439, 0.3346, 0.3207, 0.3859, 0.3201, 0.4442],\n",
       "         [0.3439, 1.0000, 0.5266, 0.6166, 0.3170, 0.4950, 0.4451],\n",
       "         [0.3346, 0.5266, 1.0000, 0.6040, 0.4056, 0.5129, 0.4597],\n",
       "         [0.3207, 0.6166, 0.6040, 1.0000, 0.3354, 0.4991, 0.4417],\n",
       "         [0.3859, 0.3170, 0.4056, 0.3354, 1.0000, 0.3729, 0.4068],\n",
       "         [0.3201, 0.4950, 0.5129, 0.4991, 0.3729, 1.0000, 0.4309],\n",
       "         [0.4442, 0.4451, 0.4597, 0.4417, 0.4068, 0.4309, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.5090, 0.1785, 0.4134, 0.4138, 0.3193, 0.4591],\n",
       "         [0.5090, 1.0000, 0.3123, 0.4945, 0.4886, 0.3843, 0.4931],\n",
       "         [0.1785, 0.3123, 1.0000, 0.3699, 0.3646, 0.2931, 0.3554],\n",
       "         [0.4134, 0.4945, 0.3699, 1.0000, 0.5597, 0.4589, 0.5631],\n",
       "         [0.4138, 0.4886, 0.3646, 0.5597, 1.0000, 0.4896, 0.5314],\n",
       "         [0.3193, 0.3843, 0.2931, 0.4589, 0.4896, 1.0000, 0.4992],\n",
       "         [0.4591, 0.4931, 0.3554, 0.5631, 0.5314, 0.4992, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.3682, 0.3463, 0.3335, 0.4043, 0.3189, 0.3325],\n",
       "         [0.3682, 1.0000, 0.5911, 0.6274, 0.4662, 0.4764, 0.4396],\n",
       "         [0.3463, 0.5911, 1.0000, 0.5884, 0.3576, 0.4918, 0.4220],\n",
       "         [0.3335, 0.6274, 0.5884, 1.0000, 0.4379, 0.4488, 0.4262],\n",
       "         [0.4043, 0.4662, 0.3576, 0.4379, 1.0000, 0.3444, 0.3508],\n",
       "         [0.3189, 0.4764, 0.4918, 0.4488, 0.3444, 1.0000, 0.3530],\n",
       "         [0.3325, 0.4396, 0.4220, 0.4262, 0.3508, 0.3530, 1.0000]]],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  0.2169,  0.2944,  0.1946,  0.2718,  0.2641,  0.2399],\n",
       "         [ 0.2169,  1.0000,  0.2441,  0.3328,  0.4282,  0.2920,  0.2742],\n",
       "         [ 0.2944,  0.2441,  1.0000,  0.3478,  0.3342,  0.2168,  0.1938],\n",
       "         [ 0.1946,  0.3328,  0.3478,  1.0000,  0.3746,  0.3116,  0.2132],\n",
       "         [ 0.2718,  0.4282,  0.3342,  0.3746,  1.0000,  0.2903,  0.1650],\n",
       "         [ 0.2641,  0.2920,  0.2168,  0.3116,  0.2903,  1.0000,  0.3133],\n",
       "         [ 0.2399,  0.2742,  0.1938,  0.2132,  0.1650,  0.3133,  1.0000]],\n",
       "\n",
       "        [[ 1.0000, -0.2257, -0.0299, -0.1858, -0.0837, -0.1312, -0.1989],\n",
       "         [-0.2257,  1.0000,  0.0049, -0.1702, -0.1622, -0.0238, -0.1469],\n",
       "         [-0.0299,  0.0049,  1.0000, -0.1058,  0.0104, -0.1170, -0.0866],\n",
       "         [-0.1858, -0.1702, -0.1058,  1.0000, -0.1078, -0.2339, -0.2198],\n",
       "         [-0.0837, -0.1622,  0.0104, -0.1078,  1.0000, -0.1084, -0.1774],\n",
       "         [-0.1312, -0.0238, -0.1170, -0.2339, -0.1084,  1.0000, -0.1002],\n",
       "         [-0.1989, -0.1469, -0.0866, -0.2198, -0.1774, -0.1002,  1.0000]],\n",
       "\n",
       "        [[ 1.0000,  0.3815,  0.2447,  0.2722,  0.3850,  0.1746,  0.2348],\n",
       "         [ 0.3815,  1.0000,  0.3503,  0.3912,  0.4276,  0.2474,  0.3095],\n",
       "         [ 0.2447,  0.3503,  1.0000,  0.6180,  0.3506,  0.5339,  0.5888],\n",
       "         [ 0.2722,  0.3912,  0.6180,  1.0000,  0.3909,  0.5163,  0.6194],\n",
       "         [ 0.3850,  0.4276,  0.3506,  0.3909,  1.0000,  0.3591,  0.2950],\n",
       "         [ 0.1746,  0.2474,  0.5339,  0.5163,  0.3591,  1.0000,  0.5680],\n",
       "         [ 0.2348,  0.3095,  0.5888,  0.6194,  0.2950,  0.5680,  1.0000]],\n",
       "\n",
       "        [[ 1.0000, -0.2471, -0.0532, -0.1750, -0.0152, -0.1436, -0.1236],\n",
       "         [-0.2471,  1.0000,  0.1384, -0.0967, -0.1525, -0.0395, -0.1999],\n",
       "         [-0.0532,  0.1384,  1.0000, -0.1901, -0.0369,  0.0485, -0.0728],\n",
       "         [-0.1750, -0.0967, -0.1901,  1.0000, -0.0688, -0.2317, -0.2484],\n",
       "         [-0.0152, -0.1525, -0.0369, -0.0688,  1.0000, -0.1884, -0.0917],\n",
       "         [-0.1436, -0.0395,  0.0485, -0.2317, -0.1884,  1.0000, -0.1454],\n",
       "         [-0.1236, -0.1999, -0.0728, -0.2484, -0.0917, -0.1454,  1.0000]],\n",
       "\n",
       "        [[ 1.0000,  0.2519,  0.3901,  0.3167,  0.2980,  0.2472,  0.2766],\n",
       "         [ 0.2519,  1.0000,  0.5235,  0.4487,  0.4330,  0.5543,  0.4632],\n",
       "         [ 0.3901,  0.5235,  1.0000,  0.5365,  0.5083,  0.5424,  0.4139],\n",
       "         [ 0.3167,  0.4487,  0.5365,  1.0000,  0.4685,  0.3926,  0.4084],\n",
       "         [ 0.2980,  0.4330,  0.5083,  0.4685,  1.0000,  0.3563,  0.3808],\n",
       "         [ 0.2472,  0.5543,  0.5424,  0.3926,  0.3563,  1.0000,  0.4616],\n",
       "         [ 0.2766,  0.4632,  0.4139,  0.4084,  0.3808,  0.4616,  1.0000]],\n",
       "\n",
       "        [[ 1.0000, -0.2218, -0.0645, -0.1923, -0.1436, -0.1121, -0.1149],\n",
       "         [-0.2218,  1.0000, -0.0888, -0.0734, -0.1604, -0.0407, -0.1684],\n",
       "         [-0.0645, -0.0888,  1.0000, -0.0829,  0.1131, -0.1361, -0.0759],\n",
       "         [-0.1923, -0.0734, -0.0829,  1.0000, -0.1494, -0.2427, -0.2519],\n",
       "         [-0.1436, -0.1604,  0.1131, -0.1494,  1.0000, -0.1361, -0.0736],\n",
       "         [-0.1121, -0.0407, -0.1361, -0.2427, -0.1361,  1.0000, -0.1947],\n",
       "         [-0.1149, -0.1684, -0.0759, -0.2519, -0.0736, -0.1947,  1.0000]],\n",
       "\n",
       "        [[ 1.0000,  0.4288,  0.4809,  0.4603,  0.3444,  0.1951,  0.2873],\n",
       "         [ 0.4288,  1.0000,  0.5642,  0.5542,  0.4294,  0.2715,  0.2996],\n",
       "         [ 0.4809,  0.5642,  1.0000,  0.6316,  0.5043,  0.3153,  0.3473],\n",
       "         [ 0.4603,  0.5542,  0.6316,  1.0000,  0.4306,  0.2526,  0.2972],\n",
       "         [ 0.3444,  0.4294,  0.5043,  0.4306,  1.0000,  0.2713,  0.2282],\n",
       "         [ 0.1951,  0.2715,  0.3153,  0.2526,  0.2713,  1.0000,  0.2846],\n",
       "         [ 0.2873,  0.2996,  0.3473,  0.2972,  0.2282,  0.2846,  1.0000]],\n",
       "\n",
       "        [[ 1.0000, -0.2617, -0.1176, -0.2842, -0.0973, -0.1268, -0.1016],\n",
       "         [-0.2617,  1.0000,  0.0390,  0.0045, -0.2752, -0.0478, -0.2284],\n",
       "         [-0.1176,  0.0390,  1.0000, -0.0742, -0.0061, -0.1339, -0.1135],\n",
       "         [-0.2842,  0.0045, -0.0742,  1.0000, -0.2125, -0.2030, -0.2919],\n",
       "         [-0.0973, -0.2752, -0.0061, -0.2125,  1.0000, -0.0978, -0.0579],\n",
       "         [-0.1268, -0.0478, -0.1339, -0.2030, -0.0978,  1.0000, -0.1920],\n",
       "         [-0.1016, -0.2284, -0.1135, -0.2919, -0.0579, -0.1920,  1.0000]],\n",
       "\n",
       "        [[ 1.0000,  0.5090,  0.1785,  0.4134,  0.4138,  0.3193,  0.4591],\n",
       "         [ 0.5090,  1.0000,  0.3123,  0.4945,  0.4886,  0.3843,  0.4931],\n",
       "         [ 0.1785,  0.3123,  1.0000,  0.3699,  0.3646,  0.2931,  0.3554],\n",
       "         [ 0.4134,  0.4945,  0.3699,  1.0000,  0.5597,  0.4589,  0.5631],\n",
       "         [ 0.4138,  0.4886,  0.3646,  0.5597,  1.0000,  0.4896,  0.5314],\n",
       "         [ 0.3193,  0.3843,  0.2931,  0.4589,  0.4896,  1.0000,  0.4992],\n",
       "         [ 0.4591,  0.4931,  0.3554,  0.5631,  0.5314,  0.4992,  1.0000]],\n",
       "\n",
       "        [[ 1.0000, -0.3004, -0.0824, -0.2867, -0.1075, -0.0688, -0.1378],\n",
       "         [-0.3004,  1.0000,  0.1034, -0.0184, -0.1950, -0.0960, -0.2346],\n",
       "         [-0.0824,  0.1034,  1.0000, -0.0926, -0.0977, -0.1478, -0.1152],\n",
       "         [-0.2867, -0.0184, -0.0926,  1.0000, -0.1661, -0.2221, -0.2674],\n",
       "         [-0.1075, -0.1950, -0.0977, -0.1661,  1.0000, -0.1272, -0.0887],\n",
       "         [-0.0688, -0.0960, -0.1478, -0.2221, -0.1272,  1.0000, -0.1907],\n",
       "         [-0.1378, -0.2346, -0.1152, -0.2674, -0.0887, -0.1907,  1.0000]]],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
